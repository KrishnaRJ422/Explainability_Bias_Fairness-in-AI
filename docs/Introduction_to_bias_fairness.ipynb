{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color='red'>Introduction to bias and fairness in ML models</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Measures of fairness:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tMetrics based on base rates:\n",
    "o\tDisparate Impact(DI): ratio between the probability of unprivileged group gets favorable prediction and the probability of privileged group gets favorable prediction\n",
    "o\tStatistical Parity Difference(SPD): similar to DI but instead of ratios, differences is calculated\n",
    "•\tMetrics based on group conditioned rates:\n",
    "o\tEqual Opportunity Difference (EOD): difference between TPR values of unprivileged and privileged groups.\n",
    "o\tAverage Odds Difference (AOD): average of false positive rate difference between FPR of unprivileged and privileged groups and TPR of unprivileged and privileged groups.\n",
    "o\tError Rate Difference (ERD): \n",
    "o\tError rate ERR=FPR+FNR\n",
    "o\tERD= ERR(U)-ERR(P)\n",
    "•\tMetrics based on individual fairness:\n",
    "o\tConsistency (CNT): measures how similar are the predictions when the instances are similar.\n",
    "o\tTheil Index (TI) / Entropy Index: Measures both group and individual fairness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Bias Mitigation Techniques:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tPre-Processing Algorithms: do not change the model, only works on dataset before training\n",
    "o\tReweighing: different weights are assigned to reduce effect of favouritism of a specified group.\n",
    "o\tDisparate Impact Remover (DIR): based on the concept of DI. It modifies the value of protected attribute to remove distinguishing factors\n",
    "•\tIn-Processing Algorithms: modify ml model\n",
    "o\tAdversarial Debiasing: introduces backward feedback(negative gradient) for predicting protected attribute which is achieved by using adversarial model that learns from difference between protected and other attributes.\n",
    "o\tPrejudice Remover Regularizer: if a model’s decision is dependent on a protected attribute, it is called a direct prejudice. To handle this , we can remove this protected variable or regulate its effect on ml model. This regularization is used under this approach where a regularizer is implemented that computes the effect of protected attribute.\n",
    "•\tPost-Processing Algorithms: modifies the predicted results instead of ml models or input data\n",
    "o\tEqualized odds (E): it changes the output labels to optimize EOD metric. A linear program is solved to obtain probabilities of modifying prediction.\n",
    "o\tCalibrated Equalized odds: this optimizes EOD metric by using calibrated prediction score produced by classifier.\n",
    "o\tReject Option Classification: it favors the instances in privileged group over unprivileged ones that lie in the decision boundary with high uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source:\n",
    "Do the Machine Learning Models on a Crowd Sourced Platform Exhibit Bias? An Empirical Study on Model Fairness \n",
    "Sumon Biswas Dept. of Computer Science, Iowa State University Ames, IA, USA sumon@iastate.edu\n",
    " Hridesh Rajan Dept. of Computer Science, Iowa State University Ames, IA, USA hridesh@iastate.edu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
